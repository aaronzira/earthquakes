{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlating Oil and Gas Drilling with Seismicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*Skip ahead to the map for 2015 [here](https://rawgit.com/aaronzira/earthquakes/master/map.html), which is too large to keep in this notebook.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having studied geology as an undergraduate, and having personally experienced seismic events of moment magnitudes [6.7](https://en.wikipedia.org/wiki/1994_Northridge_earthquake) and [9.0](https://en.wikipedia.org/wiki/2011_T%C5%8Dhoku_earthquake_and_tsunami), the prospect of working with earthquake data was very enticing for me. Articles such as [this one](http://www.scientificamerican.com/article/drilling-for-earthquakes/) from Scientific American highlight the dramatic increase in number of earthquakes in the Central US over the last few decades (as seen in the USGS image below), and their potential correlations with drilling operations in the area: \n",
    "\n",
    ">\"Researchers at the USGS and other institutions have tied earthquake surges in eight states, including **Texas, Oklahoma, Ohio, Kansas and Arkansas,** to oil and gas operations...\"\n",
    "\n",
    "[<img src=\"http://earthquake.usgs.gov/research/induced/images/hockey-stick.gif\" alt=\"Cumulative central US earthquakes 1973-2016\" title=\"Cumulative central US earthquakes 1973-2016\" style=\"width: 650px;\"/>](http://earthquake.usgs.gov/research/induced/)\n",
    "\n",
    "My goal with this project was twofold -- merge disparate data sources to:\n",
    "1. create an easy-to-digest visualization for a casual observer to learn from\n",
    "2. make that data available for future machine learning projects\n",
    "\n",
    "The [USGS earthquake catalog](http://earthquake.usgs.gov/fdsnws/event/1/) seemed an excellent source of unbounded data that could be compared against static data on oil and gas wells to accomplish my goals. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "In order to ensure compatibility with [Apache Spark](http://spark.apache.org/), this project makes use of a Python 2 kernel (2.7.12)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function,unicode_literals\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import numpy as np\n",
    "import requests\n",
    "from scipy import spatial\n",
    "from shapely.geometry import shape,Point\n",
    "import folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wells: create tables, find counties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this great [index of wells data by state](http://pmc.ucsc.edu/~brodsky/wellindex.html), I was able to find which states had data available and where. Many states charge for the data or have broken links, but thankfully Colorado, Kansas, and Oklahoma had theirs readily available and are contiguous, so that seemed a perfect place to start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download zip/shape files, read in as Pandas DataFrame objects\n",
    "\n",
    "The .txt and .xlsx files of wells from Kansas and Oklahoma are no problem for [Pandas](http://pandas.pydata.org/)' `.read_csv()` and `.read_excel()` functions, but the GIS shapefiles for Colorado needed to be converted to something that Python could work with more easily.\n",
    "\n",
    "[Colorado](http://cogcc.state.co.us/documents/data/downloads/gis/WELLS_SHP.ZIP) -> converted to .csv file with [mapshaper](http://www.mapshaper.org/)\n",
    "\n",
    "[Oklahoma](http://pmc.ucsc.edu/~ivmiller/UIC.xlsx.zip)\n",
    "\n",
    "[Kansas](http://www.kgs.ku.edu/PRS/Ora_Archive/ks_wells.zip \"ks_wells.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "co = pd.read_csv('./eq_project/wells/wells.csv')\n",
    "ks = pd.read_csv('./eq_project/wells/ks_wells.txt')\n",
    "ok = pd.read_excel('./eq_project/wells/UIC.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Each state table has many features, many of which were not used for this phase of the project\n",
    "print('Colorado:',co.columns.values)\n",
    "print('Kansas:',ks.columns.values)\n",
    "print('Oklahoma:',ok.columns.values)\n",
    "```\n",
    "```\n",
    "Colorado: ['API' 'API_Label' 'Operator' 'Well_Title' 'Facil_Id' 'Facil_Type'\n",
    " 'Facil_Stat' 'Operat_Num' 'Well_Num' 'Well_Name' 'Field_Code' 'Dist_N_S'\n",
    " 'Dir_N_S' 'Dist_E_W' 'Dir_E_W' 'Qtr_Qtr' 'Section' 'Township' 'Range'\n",
    " 'Meridian' 'Latitude' 'Longitude' 'Ground_Ele' 'Utm_X' 'Utm_Y' 'Loc_Qual'\n",
    " 'Field_Name' 'Api_Seq' 'API_County' 'Loc_ID' 'Loc_Name' 'Spud_Date'\n",
    " 'Citing_Typ' 'Max_MD' 'Max_TVD']\n",
    "Kansas: ['KID' 'API_NUMBER' 'API_NUM_NODASH' 'LEASE' 'WELL' 'FIELD' 'LATITUDE'\n",
    " 'LONGITUDE' 'LONG_LAT_SOURCE' 'TOWNSHIP' 'TWN_DIR' 'RANGE' 'RANGE_DIR'\n",
    " 'SECTION' 'SPOT' 'FEET_NORTH' 'FEET_EAST' 'FOOT_REF' 'ORIG_OPERATOR'\n",
    " 'CURR_OPERATOR' 'ELEVATION' 'ELEV_REF' 'DEPTH' 'PRODUCE_FORM' 'IP_OIL'\n",
    " 'IP_GAS' 'IP_WATER' 'PERMIT' 'SPUD' 'COMPLETION' 'PLUGGING' 'MODIFIED'\n",
    " 'OIL_KID' 'OIL_DOR_ID' 'GAS_KID' 'GAS_DOR_ID' 'KCC_DOCKET' 'STATUS'\n",
    " 'STATUS2' 'COMMENTS']\n",
    "Oklahoma: ['API_COUNTY' 'API_NUMBER' 'LEASE_NAME' 'WELL_NUMBER' 'DECIMAL_LAT'\n",
    " 'DECIMAL_LONG' 'YEAR_1012A' 'PACKERDPTH' 'JAN' 'PRESSURE' 'VOLUME' 'FEB'\n",
    " 'PRESSURE_1' 'VOLUME_1' 'MARCH' 'PRESSURE_2' 'VOLUME_2' 'APRIL'\n",
    " 'PRESSURE_3' 'VOLUME_3' 'MAY' 'PRESSURE_4' 'VOLUME_4' 'JUNE' 'PRESSURE_5'\n",
    " 'VOLUME_5' 'JULY' 'PRESSURE_6' 'VOLUME_6' 'AUGUST' 'PRESSURE_7' 'VOLUME_7'\n",
    " 'SEPT' 'PRESSURE_8' 'VOLUME_8' 'OCT' 'PRESSURE_9' 'VOLUME_9' 'NOV'\n",
    " 'PRESSURE_10' 'VOLUME_10' 'DEC' 'PRESSURE_11' 'VOLUME_11' 'Unnamed: 44'\n",
    " 'Unnamed: 45' 'Unnamed: 46' 'Unnamed: 47' 'Unnamed: 48' 1977 3141810]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deciding on relevant features\n",
    "\n",
    "I had originally intended to make use of features such as depth, time since spudding, and the name on the lease, but ultimately determined that because I had less than two weeks for the project, [API number](https://en.wikipedia.org/wiki/API_well_number), location (latitude and longitude), and county were the only important features for this project. While Colorado and Oklahoma have columns of API codes that indicate the county in which the surface of a well is found, for Kansas there is only information on Township, which was not relevant to me. I ended up cross checking the latitude and longitude of each well in that state against a [GeoJSON file](http://geojson.org/) that essentially splits the state into one polygon for each county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keep important features only\n",
    "co = co[['API','API_County','Latitude','Longitude']]\n",
    "ks = ks[['API_NUMBER','LATITUDE','LONGITUDE']]\n",
    "ok = ok[['API_NUMBER','API_COUNTY','DECIMAL_LAT','DECIMAL_LONG']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "print('Colorado:\\n\\t',co.columns.values,'\\n\\t\\tx',co.shape[0],'wells')\n",
    "print('Kansas:\\n\\t',ks.columns.values,'\\n\\t\\tx',ks.shape[0],'wells')\n",
    "print('Oklahoma:\\n\\t',ok.columns.values,'\\n\\t\\tx',ok.shape[0],'wells')\n",
    "\n",
    "```\n",
    "```\n",
    "Colorado:\n",
    "\t ['API' 'API_County' 'Latitude' 'Longitude'] \n",
    "\t\tx 110051 wells\n",
    "Kansas:\n",
    "\t ['API_NUMBER' 'LATITUDE' 'LONGITUDE'] \n",
    "\t\tx 474515 wells\n",
    "Oklahoma:\n",
    "\t ['API_NUMBER' 'API_COUNTY' 'DECIMAL_LAT' 'DECIMAL_LONG'] \n",
    "\t\tx 144941 wells\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine county of each well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The well databases of the three states I retrieved did not include the county of a given well, and I needed that in order to generate a [choropleth map](https://en.wikipedia.org/wiki/Choropleth_map) of the three states with counties shaded based on the number of wells within them. The Oklahoma and Colorado databases contained columns with the API county code for each well, which I converted to county name using [this api codes pdf](https://dl.ppdm.org/dl/796) and the [PyPDF2 module](https://pythonhosted.org/PyPDF2/) to extract the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdf = PyPDF2.PdfFileReader(open('./eq_project/codes.pdf',\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def api_code_extractor(pages,state,first_line,last_line):\n",
    "    \n",
    "    \"\"\"Extracts the codes and corresponding county names from the API codes PDF \n",
    "    for a given state, and returns a data frame of the results.  The relevant \n",
    "    pages and line numbers must be determined manually by examining the lines \n",
    "    retrieved by the PyPDF2 module.  The formatting of county names is designed \n",
    "    to match the keys in the state GeoJSON files from CivicDashboards at \n",
    "    http://catalog.opendata.city/dataset/\n",
    "        \n",
    "    Args:\n",
    "        pages (list): Ordered page numbers for the current state.\n",
    "        state (str): The two-letter state code of the current state.\n",
    "        first_line (int): The line number of the first API code for the \n",
    "            current state (must be determined manually).\n",
    "        last_line (int): The line number of the last county name for the \n",
    "            current state (must be determined manually).\n",
    "    \n",
    "    Returns:\n",
    "        pandas DataFrame: A table for the current state with columns of codes \n",
    "            and county names.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    dic = {}\n",
    "    pg_count = len(pages)\n",
    "    \n",
    "    for loop,page_num in enumerate(pages):\n",
    "        page = pdf.getPage(page_num)        \n",
    "        \n",
    "        # These logical checks are overkill for the states I used, but are \n",
    "        # put in place in hopes that data for other states would be added \n",
    "        # to the analysis. \n",
    "        \n",
    "        # Assume last page by default\n",
    "        first = 10\n",
    "        last = last_line\n",
    "\n",
    "        # Any page other than last\n",
    "        if pg_count-(loop+1) > 0:\n",
    "            last = -1\n",
    "            \n",
    "            # First page only\n",
    "            if not dic: \n",
    "                # If state has less than one full page of counties\n",
    "                if pg_count == 1:\n",
    "                    last = last_line\n",
    "                first = first_line\n",
    "        \n",
    "        # Zip together codes and matching counties, update dictionary with \n",
    "        # codes as keys and properly formatted county names as values.\n",
    "        for code,county in zip(page.extractText().splitlines()[first:last:4],\n",
    "                               page.extractText().splitlines()[first+1:last:4]):\n",
    "            dic[int(code)] = ' '.join([word[0]+word[1:].lower() for word in \n",
    "                                county.split()])+' County, {}'.format(state)\n",
    "    \n",
    "    df = pd.DataFrame.from_dict({'Code':dic}).reset_index()\n",
    "    df.columns = ['Code','County']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After retrieving the codes and corresponding counties and storing them as data frames, I joined them with the Colorado and Oklahoma tables such that the county is included with the other relevant data for each well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "co_codes = api_code_extractor([5,6],'CO',first_line=38,last_line=86)\n",
    "ok_codes = api_code_extractor([44,45],'OK',first_line=94,last_line=192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ok['COUNTY'] = ok.merge(ok_codes,how='left',\n",
    "                        left_on='API_COUNTY',right_on='Code')['County']\n",
    "co['County'] = co.merge(co_codes,how='left',\n",
    "                        left_on='API_County',right_on='Code')['County']\n",
    "ok.dropna(inplace=True)\n",
    "co.dropna(inplace=True)\n",
    "ks.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from matching the formatting \n",
    "\n",
    ">'[COUNTY NAME] County, [STATE ABBREVIATION]'\n",
    "\n",
    "in the GeoJSON files linked to in the section below on creating the map, there were two small inconsistencies between the PDF and the key names, so I manually replaced those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ok['COUNTY'] = ok['COUNTY'].replace({\n",
    "                        'Mc Clain County, OK':'McClain County, OK',\n",
    "                        'Mc Intosh County, OK':'McIntosh County, OK'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining counties for Kansas wells was not as straightforward. Thanks to explanations from users 'Zebs' and 'wf.' on [this stack overflow thread](http://stackoverflow.com/questions/20776205/point-in-polygon-with-geojson-in-python), I was able to use the [Shapely](https://pypi.python.org/pypi/Shapely) module and read in the boundaries of Kansas' county polygons from [this GeoJSON file](http://catalog.opendata.city/fa_IR/dataset/kansas-counties-polygon/resource/815cebfa-5666-4f7d-943c-f53945a4347e \"Kansas GeoJSON file\"), and loop through each point (latitude and longitude of a well) to determine which polygon (and therefore county) contained it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dictionary of Kansas counties with county name as key, \n",
    "# corresponding boundary outline (as Shapely shape object) as value.\n",
    "polygons = {}\n",
    "\n",
    "with open('./eq_project/counties/kansas.geojson','r') as f:\n",
    "    js = json.load(f)\n",
    "\n",
    "for feature in js['features']:\n",
    "    polygons[feature['properties']['name']] = shape(feature['geometry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionary of Shapely point objects of each well based on its lat and long, \n",
    "# using the well's API number as the key, and the long/lat coordinates as the \n",
    "# value (lat and long are flipped with Shapely).\n",
    "pts = {}\n",
    "for point in ks.values:\n",
    "    pts[point[0]]=(Point(point[[2,1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "While I'd rather have not used nested ```for``` loops below, for my purposes 3-4 minutes was \n",
    "not a big price to pay, and the logic is simple to follow: looking at one \n",
    "well at a time, loop through all counties until finding the first one that \n",
    "contains it, and assign to a new dictionary a key of that well's API number, \n",
    "and a value of the county in which it was found.  The ```break``` in the loop \n",
    "saves time, as only one county will contain any one well, so there is no \n",
    "reason to continue searching after finding a match. Given more time, I would \n",
    "have chosen a data structure that would speed this up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ks_well_counties = {}\n",
    "for api,coordinates in pts.items():\n",
    "    for county,polygon in polygons.items():\n",
    "        if polygon.contains(coordinates):\n",
    "            ks_well_counties[api] = county\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Kansas API and county database\n",
    "ks_with_counties = pd.DataFrame(\n",
    "                ([api,county] for api,county in ks_well_counties.items()),\n",
    "                columns=['API','County'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In creating the final map I could not ignore missing keys in the GeoJSON file, so I had to manually set values for those counties to 0; i.e., there are no wells in the following counties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_wells = pd.DataFrame({'County': np.array([\n",
    "                        'Johnston County, OK','Ottawa County, OK',\n",
    "                        'Pushmataha County, OK','Adair County, OK',\n",
    "                        'Cherokee County, OK','Delaware County, OK',\n",
    "                        'Choctaw County, OK','McCurtain County, OK',\n",
    "                        'Teller County, CO','Rio Grande County, CO',\n",
    "                        'Summit County, CO','Mineral County, CO',\n",
    "                        'Clear Creek County, CO','Lake County, CO',\n",
    "                        'San Juan County, CO','Hinsdale County, CO',\n",
    "                        'Gilpin County, CO']),\n",
    "                        'Number': np.array([0]*17)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out cleaned tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I had the wells tables cleaned up as I wanted them, so I wrote them out to CSV (as .txt) files to have as backups and for working with [Spark](http://spark.apache.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ok.to_csv('./eq_project/CSVs/oklahoma.txt',header=False,index=False)\n",
    "co.to_csv('./eq_project/CSVs/colorado.txt',header=False,index=False)\n",
    "ks.to_csv('./eq_project/CSVs/kansas.txt',header=False,index=False)\n",
    "ks_with_counties.to_csv('./eq_project/CSVs/kansas_counties.txt',header=False,index=False)\n",
    "no_wells.to_csv('./eq_project/CSVs/no_wells.txt',header=False,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though I only accessed the data locally, Apache Spark allows one to take advantage of the very powerful, fault-tolerant [RDD](https://spark.apache.org/docs/0.8.1/api/core/org/apache/spark/rdd/RDD.html \"Resilient Distributed Dataset\") and associated capabilities, crucially to perform parallel operations across a cluster of machines on massive amounts of data, without needing to think much about optimization. In this case, it was for practice. In order to start a Spark-enabled IPython notebook from the terminal, one can run:\n",
    "\n",
    "```\n",
    "IPYTHON_OPTS=\"notebook\" pyspark```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Test connection to Spark context:\n",
    "sc\n",
    "```\n",
    "```\n",
    "<pyspark.context.SparkContext at 0x1090a3610>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in CSV files as RDDs\n",
    "oklahoma = sc.textFile('./eq_project/CSVs/oklahoma.txt')\n",
    "colorado = sc.textFile('./eq_project/CSVs/colorado.txt')\n",
    "kansas = sc.textFile('./eq_project/CSVs/kansas.txt')\n",
    "kansas_counties = sc.textFile('./eq_project/CSVs/kansas_counties.txt')\n",
    "no_wells_rdd = sc.textFile('./eq_project/CSVs/no_wells.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get RDDs of just the data I needed at this point, I dropped the API county code columns from the Colorado and Oklahoma tables, and extracted properly typed data from each line. The `API` field for Colorado included the county code as a prefix, so I only took the last five digits. For Kansas, the `API_NUMBER` field included several extra codes, the longest of which was the API number alone, so I only kept that. In addtion, it was at this point that I joined the two Kansas tables (one with API and location, the other with API and county)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adjust formatting to take advantage of Spark's key-value pair operations\n",
    "kansas = kansas.map(\n",
    "    lambda line:line.split(',')).map(\n",
    "    lambda _list:(str(_list[0]),float(_list[1]),float(_list[2]))).map(\n",
    "    lambda row:(row[0],[row[1],row[2]]))\n",
    "\n",
    "kansas_counties = kansas_counties.map(\n",
    "    lambda line:line.split(',')).map(\n",
    "    lambda _list:(str(_list[0]),str(_list[1][1:]+','+_list[2][:-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kansas = kansas.join(kansas_counties).map(\n",
    "    lambda _list:[int(_list[0].split('-')[\n",
    "                    np.argmax(map(len,_list[0].split('-')))]),\n",
    "                  float(_list[1][0][0]),float(_list[1][0][1]),\n",
    "                  str(_list[1][1])])\n",
    "\n",
    "colorado = colorado.map(\n",
    "    lambda line:line.split(',')).map(\n",
    "    lambda _list:[int(_list[0][-5:]),\n",
    "                  float(_list[2]),float(_list[3]),\n",
    "                  str(','.join((_list[4],_list[5])))[1:-1]])\n",
    "\n",
    "oklahoma = oklahoma.map(\n",
    "    lambda line:line.split(',')).map(\n",
    "    lambda _list:[int(_list[0]),\n",
    "                  float(_list[2]),float(_list[3]),\n",
    "                  str(','.join((_list[4],_list[5])))[1:-1]])\n",
    "\n",
    "no_wells_rdd = no_wells_rdd.map(\n",
    "    lambda line:(str(line[1:-3]),int(line[-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Check formatting\n",
    "print(oklahoma.first())\n",
    "print(kansas.first())\n",
    "print(colorado.first())\n",
    "```\n",
    "```\n",
    "[21680, 34.6907726, -97.057519, 'Garvin County, OK']\n",
    "[19926, 37.91039, -95.09883, 'Allen County, KS']\n",
    "[8888, 40.419416, -102.379999, 'Yuma County, CO']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then combined the three states RDDs into one, and determined the number of times each county name appears (i.e., how many wells each county has) to use in shading the choropleth map. Here I also needed to include the counties in which there were no wells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All three states combined\n",
    "wells_db = sc.union([oklahoma,colorado,kansas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of wells per county\n",
    "all_wells_per_county = wells_db.map(\n",
    "    lambda row:(row[3],1)).reduceByKey(\n",
    "    lambda x,y:x+y).union(no_wells_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Confirm number of counties\n",
    "all_wells_per_county.count()\n",
    "```\n",
    "```\n",
    "247\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find earthquakes near wells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data on all of the wells was in a cleaned, convenient format, the next step was to find a data structure that could hold this information and allow for calculation of distances between earthquakes and the wells. After a number of dead ends, I settled on the [K-D tree](https://en.wikipedia.org/wiki/K-d_tree), which would allow me to see each well as a point in latitude-longitude space, so comparing against coordinates of an earthquake's epicenter would be very simple. For this project I used the `KDTree` object from [scipy.spatial](http://docs.scipy.org/doc/scipy/reference/spatial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create directory and K-D tree of all wells in the three states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wells_map = {}\n",
    "for api,lat,lng,county in wells_db.collect():\n",
    "    wells_map[lat,lng] = api\n",
    "    \n",
    "locations = (wells_map.keys())\n",
    "wells_tree = spatial.KDTree(locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find nearest well from an earthquake\n",
    "\n",
    "This function is used within another (`eq_to_map()`) that is defined below, in returning pertinent information on the nearest well to an earthquake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nearest_well(point,tree,directory):\n",
    "    \"\"\"Returns information on the nearest well to a given earthquake.\n",
    "    \n",
    "    Args:\n",
    "        point (list): Latitude and longitude of an earthquake.\n",
    "        tree (Scipy spatial K-D tree): Well locations to search.\n",
    "        directory (dict): Lat/long coordinates of wells in the K-D tree and \n",
    "            corresponding API numbers.\n",
    "    \n",
    "    Returns:\n",
    "        str: Distance in km from the earthquake to the nearest well.  The \n",
    "            distance is calculated by converting one decimal degree to an \n",
    "            approximate 111.32km.\n",
    "    \"\"\"\n",
    "    \n",
    "    distance,index = tree.query(point)\n",
    "    return str(distance*111.32) +' km '+ 'from Well # '+ \\\n",
    "        str(directory[tree.data[index][0],tree.data[index][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Read in earthquakes data from USGS\n",
    "\n",
    "This function retrieves recent earthquake data from the USGS, and renders it usable by `eq_to_map()` to plot on the choropleth maps created below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quake_to_rdd(start = datetime.datetime.fromtimestamp(\n",
    "                    time.time()-86400).strftime('%Y-%m-%dT%H:%M:%S'),\n",
    "                stop = datetime.datetime.fromtimestamp(\n",
    "                    time.time()).strftime('%Y-%m-%dT%H:%M:%S'),\n",
    "                magnitude = 1.5, lats = [33,41.5], longs = [-109.5,-94]):    \n",
    "    \"\"\"Creates an RDD of worldwide earthquakes within a given time range, above \n",
    "    a given magnitude, and bounded by given latitudes and longitudes.\n",
    "    \n",
    "    Args:\n",
    "        start (str): YYYY-MM-DD (with optional HH:MM:SS, joined by 'T' without \n",
    "            quotes) of first date to search.  Defaults to one day \n",
    "            (86400 seconds) before present.\n",
    "        stop (str): YYYY-MM-DD (with optional HH:MM:SS, joined by 'T' without \n",
    "            quotes) of last date to search.  Defaults to present.\n",
    "        magnitude (float): Minimum magnitude to search.  Defaults to 1.5.\n",
    "        lats (list): Minimum and maximum latitudes to search. \n",
    "            Default to 33 and 41.5.\n",
    "        longs (list): Minimum and maximum longitudes to search. \n",
    "            Default to -109.5 and -94.\n",
    "    \n",
    "    Returns:\n",
    "        Pyspark RDD: Most important info on each earthquake returned from the \n",
    "            search, in JSON format.\n",
    "    \"\"\"\n",
    "    \n",
    "    url = ('http://earthquake.usgs.gov/fdsnws/event/1'\n",
    "            '/query?format=geojson&starttime={0}&endtime={1}'\n",
    "            '&minmagnitude={2}'\n",
    "            '&minlatitude={3}&maxlatitude={4}'\n",
    "            '&minlongitude={5}&maxlongitude={6}'\n",
    "          ).format(start,stop,magnitude,lats[0],lats[1],longs[0],longs[1])\n",
    "    results = []\n",
    "    for event in requests.get(url).json()['features']:\n",
    "        earthquake = {}\n",
    "        # Earthquakes are returned with longitude first, latitude second.\n",
    "        earthquake['long'], earthquake['lat'], earthquake['depth'] = \\\n",
    "            event['geometry']['coordinates'][0], \\\n",
    "            event['geometry']['coordinates'][1], \\\n",
    "            event['geometry']['coordinates'][2]\n",
    "        earthquake['title'] = event['properties']['title']\n",
    "        earthquake['magnitude'] = event['properties']['mag']\n",
    "        # Earthquakes are returned from USGS in terms of milliseconds since \n",
    "        # 1 January 1970, requiring division by 1000 to match formatting in \n",
    "        # Python's datetime module.\n",
    "        earthquake['timestamp'] = \\\n",
    "            datetime.datetime.fromtimestamp(\n",
    "                event['properties']['time']/1000).strftime('%c')\n",
    "        \n",
    "        results.append(earthquake)\n",
    "        \n",
    "    return sc.parallelize(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Test with earthquakes from the past 24 hours, examine first three\n",
    "last_day = quake_to_rdd()\n",
    "last_day.take(3)\n",
    "```\n",
    "```\n",
    "[{u'depth': 5,\n",
    "  u'lat': 35.6625,\n",
    "  u'long': -97.1556,\n",
    "  u'magnitude': 3.5,\n",
    "  u'timestamp': 'Mon Aug 15 23:08:14 2016',\n",
    "  u'title': u'M 3.5 - 3km E of Luther, Oklahoma'},\n",
    " {u'depth': 5,\n",
    "  u'lat': 36.4566,\n",
    "  u'long': -98.7433,\n",
    "  u'magnitude': 2.6,\n",
    "  u'timestamp': 'Mon Aug 15 17:07:19 2016',\n",
    "  u'title': u'M 2.6 - 31km NW of Fairview, Oklahoma'},\n",
    " {u'depth': 3.94,\n",
    "  u'lat': 37.242,\n",
    "  u'long': -97.6586667,\n",
    "  u'magnitude': 1.81,\n",
    "  u'timestamp': 'Mon Aug 15 07:12:10 2016',\n",
    "  u'title': u'M 1.8 - 16km S of Conway Springs, Kansas'}]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create choropleth map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After exploring different options of map types and reading a few great tutorials such as [this](https://blog.dominodatalab.com/creating-interactive-crime-maps-with-folium/) and [this](https://ocefpaf.github.io/python4oceanographers/blog/2015/08/24/choropleth/), I decided that [Folium's](https://github.com/python-visualization/folium) choropleth map was best suited to my needs, as I wanted county-level aggregate numbers on wells, overlaid by interactive map markers for nearby earthquakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State GeoJSON files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kansas file is the same as used above to determine the county of each well. All three states' files contain coordinates that bound each county within them. For this choropleth map, each county is shaded in proportion to the number of wells within its borders.\n",
    "\n",
    "[Colorado](http://catalog.opendata.city/dataset/colorado-counties-polygon/resource/c9ddc844-6d01-4c7c-8c98-df932ea94597) -> 'eq_project/counties/colorado.geojson'\n",
    "\n",
    "[Kansas](http://catalog.opendata.city/fa_IR/dataset/kansas-counties-polygon/resource/815cebfa-5666-4f7d-943c-f53945a4347e) -> 'eq_project/counties/kansas.geojson'\n",
    "\n",
    "[Oklahoma](http://catalog.opendata.city/dataset/oklahoma-counties-polygon/resource/75b87ccf-da9e-464e-814b-16985041d2ca) -> 'eq_project/counties/oklahoma.geojson'\n",
    "\n",
    "As I wanted one map that showed all three states together, I merged the individual GeoJSON files with [this script](https://gist.github.com/themiurgo/8687883 \"merger.py\"), run from the terminal as follows:\n",
    "\n",
    "```shell\n",
    "python merger.py -o merged.geojson colorado.geojson oklahoma.geojson kansas.geojson\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choropleth map instance with earthquake data\n",
    "The map is centered south of Manter, KS, which is roughly at the center of the three states, and the coloring/shading scale ranges from yellow for zero wells, to green, to blue. I lowered the upper threshold to 14000 wells, as there is only one county with more than 25000 (Weld County, CO - 35649; shown below), and I wanted to group the top ten or so counties together, which is more telling than having only one county colored blue on the map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "all_wells_per_county.filter(\n",
    "    lambda (county,count):count>14000).map(\n",
    "    lambda (county,count):[count,county]).sortByKey(\n",
    "    ascending=False).collect()\n",
    "```\n",
    "```\n",
    "[(35649, 'Weld County, CO'),\n",
    " (24538, 'Carter County, OK'),\n",
    " (21747, 'Montgomery County, KS'),\n",
    " (19751, 'Allen County, KS'),\n",
    " (17037, 'Barton County, KS'),\n",
    " (16689, 'Stephens County, OK'),\n",
    " (16072, 'Miami County, KS'),\n",
    " (15898, 'Garfield County, CO'),\n",
    " (15421, 'Ellis County, KS'),\n",
    " (14999, 'Neosho County, KS'),\n",
    " (14894, 'Butler County, KS'),\n",
    " (14643, 'Woodson County, KS')]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eq_to_map(data,tree,directory,center=[37.4,-102]):\n",
    "    \"\"\"Creates interactive markers of nearby earthquakes on a choropleth map, \n",
    "    then saves and returns the map.  'Nearby' means having an epicenter \n",
    "    within 10km of a well in the K-D tree.  For a qualifying earthquake, the \n",
    "    radius of the marker is proportional to the magnitude, and clicking on the \n",
    "    marker will display relevant information about it, as retrieved by \n",
    "    get_nearest_well(). \n",
    "    \n",
    "    Args:\n",
    "        data (Pyspark RDD): Most important information on earthquakes as \n",
    "            returned by quake_to_rdd().\n",
    "        tree (Scipy spatial K-D tree): Well locations to search.\n",
    "        directory (dict): Lat/long coordinates of wells in the K-D tree and \n",
    "            corresponding API numbers.\n",
    "        center (list): Lat/long coordinates of the center of the map.  Defaults \n",
    "            to [36.7295,-102.5132].\n",
    "    \n",
    "    Returns: Folium choropleth map: States colored based on number of wells \n",
    "        within their counties, and earthquake markers plotted on top.\n",
    "    \n",
    "    Note: The map is reinstantiated with each call of the function, as Folium \n",
    "        does not provide a simple way to reset all custom markers or regenerate \n",
    "        the base map.\n",
    "    \"\"\"\n",
    "    \n",
    "    choro_map = folium.Map(location=center,zoom_start=6)\n",
    "    choro_map.choropleth(\n",
    "                        geo_str = json.dumps(json.load(open(\n",
    "                            './eq_project/counties/merged.geojson','r'))),\n",
    "                        data = all_wells_per_county.collect(),\n",
    "                        columns = ['County','Number'],\n",
    "                        key_on = 'properties.name',\n",
    "                        fill_color = 'YlGnBu',\n",
    "                        fill_opacity = 0.6,\n",
    "                        line_opacity = 0.2,\n",
    "                        threshold_scale = \n",
    "                            np.linspace(1,14000,6,dtype=int).tolist(),\n",
    "                        legend_name = 'Wells per county')\n",
    "\n",
    "    for quake in data.collect():\n",
    "        # 10km * (1 degree/111.13km) = .08998 degrees\n",
    "        if tree.query([quake['lat'],quake['long']])[0] < .08998:\n",
    "            title = quake['title']\n",
    "            time = quake['timestamp']\n",
    "            folium.CircleMarker([quake['lat'],quake['long']],\n",
    "                                popup=title+'\\n--\\n'+time+'\\n--\\n'\\\n",
    "                                        +get_nearest_well([quake['lat'],\n",
    "                                                           quake['long']],\n",
    "                                                           tree,directory),\n",
    "                                radius=quake['magnitude']*2500,\n",
    "                                color='#000000',\n",
    "                                fill_color='#ff0000'\n",
    "            ).add_to(choro_map)\n",
    "    choro_map.save('./map.html')\n",
    "    return choro_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "The .html map files are too large to upload here, but my code follows. The 2015 map can be found [here](https://rawgit.com/aaronzira/earthquakes/master/map.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This will always produce a map of the last 30 days' worth of earthquakes\n",
    "# 2592000 seconds in 30 days\n",
    "thirty_days_ago = datetime.datetime.fromtimestamp(\n",
    "                            time.time()-2592000).strftime('%Y-%m-%dT%H:%M:%S')\n",
    "last_month = quake_to_rdd(start=thirty_days_ago)\n",
    "eq_to_map(last_month,wells_tree,wells_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I thought that one interesting application of these visualizations would be to compare one year's worth of earthquakes from the eighties, nineties, 2000s, and 2010s, spaced by 10 years. \n",
    "\n",
    "Note that because of the comparatively large number of earthquakes in 2015, creating that map below takes much longer than the others (several minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eighties = quake_to_rdd(start='1985-01-01',stop='1985-12-31')\n",
    "eq_to_map(eighties,wells_tree,wells_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nineties = quake_to_rdd(start='1995-01-01',stop='1995-12-31')\n",
    "eq_to_map(nineties,wells_tree,wells_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aughts = quake_to_rdd(start='2005-01-01',stop='2005-12-31')\n",
    "eq_to_map(aughts,wells_tree,wells_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tens = quake_to_rdd(start='2015-01-01',stop='2015-12-31')\n",
    "eq_to_map(tens,wells_tree,wells_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# What next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would like to port all the data to [Postgres](https://www.postgresql.org/), and take advantage of the performance, speed, and data types and structures available there, such as K-D trees and lat/long points. In addition, my distance calculations could be made more accurate using the [Haversine formula](https://en.wikipedia.org/wiki/Haversine_formula), which determines great circle distances by taking into account the radius of the earth, instead of considering the area in question a 2-dimensional map. Getting more data is pretty much always a good idea, and it would be nice to have data on wells from many other states, especially all of those for which the USGS has established correlations between drilling and seismicity. \n",
    "\n",
    "Using the data created here, it would be interesting to apply machine learning techniques to determine if there are certain features (depth, well type, years active or inactive, direction, presence of other wells nearby, etc.) that contribute to a higher likelihood of induced seismicity in a particular area. In addition, data from a geologic map could be included to examine what types of rock are found in and around the areas of interest, potentially allowing for predictive applcations of future seismic activity related to wells anywhere. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
